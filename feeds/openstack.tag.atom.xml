<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>(blog-dump 'dciabrin) - openstack</title><link href="https://dciabrin.net/" rel="alternate"></link><link href="https://dciabrin.net/feeds/openstack.tag.atom.xml" rel="self"></link><id>https://dciabrin.net/</id><updated>2015-10-02T18:45:05+02:00</updated><subtitle>A coding and hacking diary</subtitle><entry><title>Galera boot process in Open Stack HA and manual override</title><link href="https://dciabrin.net/posts/2015/10/galera-boot-process-in-open-stack-ha-and-manual-override.html" rel="alternate"></link><published>2015-10-02T18:45:05+02:00</published><updated>2015-10-02T18:45:05+02:00</updated><author><name>Damien Ciabrini</name></author><id>tag:dciabrin.net,2015-10-02:/posts/2015/10/galera-boot-process-in-open-stack-ha-and-manual-override.html</id><summary type="html">&lt;p&gt;Deployments of OpenStack that rely on MariaDB+Galera benefit from a HA database thanks to Galera's synchronous replication. In such deployments, the Galera cluster is typically managed via Pacemaker, by means of a galera resource agent.&lt;/p&gt;
&lt;p&gt;While Galera itself has its own notion of cluster management (membership, health check, write-set â€¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Deployments of OpenStack that rely on MariaDB+Galera benefit from a HA database thanks to Galera's synchronous replication. In such deployments, the Galera cluster is typically managed via Pacemaker, by means of a galera resource agent.&lt;/p&gt;
&lt;p&gt;While Galera itself has its own notion of cluster management (membership, health check, write-set replication...), a resource agent is still necessary for Pacemaker to perform the basic cluster management duties, for example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Starting up the Galera servers on the available nodes in the cluster&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Health monitoring and recovery actions on failure (e.g. fencing)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This document describes the concepts involved in booting a Galera cluster, how the galera resource agent implements the boot process of a galera cluster, and how it can be overriden for recovery scenarios.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;h2&gt;Galera cluster overview&lt;/h2&gt;
&lt;p&gt;A Galera cluster is identified by a cluster address, stored in the configuration variable &lt;code&gt;wsrep_cluster_address&lt;/code&gt;. The value of this variable is a URI identifying all the nodes that can potentially be member of the cluster. For example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wsrep_cluster_address=gcomm://node1,node2,node3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is used by MariaDB at boot time to register to the cluster and to synchronize its local database with the cluster. The value of &lt;code&gt;wsrep_cluster_address&lt;/code&gt; conveys a special meaning which can be used to either start a cluster or rejoin it.&lt;/p&gt;
&lt;h2&gt;Galera boot process explained&lt;/h2&gt;
&lt;p&gt;Galera replicates database writes across all nodes of the cluster. A write succeeds if more than half of the nodes in the cluster acknowledge it (quorum). On success, a global counter representing the most recent transaction is incremented: this is called the last sequence number, or seqno. Desynchronized nodes or newly joining nodes will automatically sync their local database to this last sequence number.&lt;/p&gt;
&lt;p&gt;In order to restart an existing Galera cluster, one needs first to identify a node whose local database contains the latest transaction acknowleged by the cluster, i.e. the one with the biggest seqno. Once identified, MariaDB can be started on the node with option:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;wsrep_cluster_address=gcomm://
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This bootstraps a new cluster&lt;sup id="fnref:1"&gt;&lt;a class="footnote-ref" href="#fn:1"&gt;1&lt;/a&gt;&lt;/sup&gt; from this node's local state: the node becomes the new Primary partition, which means the remaining nodes will sync against this new cluster when started with &lt;code&gt;wsrep_cluster_address=gcomm://node1,node2,node3&lt;/code&gt;.  &lt;/p&gt;
&lt;h2&gt;How the resource agent boots the cluster&lt;/h2&gt;
&lt;p&gt;The resource agent encodes the process of booting a Galera cluster as a series of unitary steps; electing a bootstrap node, booting Galera servers in sequence, and marking nodes as available in the clusters. It tracks those steps via Pacemaker's multi-state resource plus various attributes stored in Pacemaker's Cluster Information Base (CIB).&lt;/p&gt;
&lt;p&gt;In order to boot or restart a Galera cluster, the resource agent needs to retrieve the last seqno of all the nodes in the clusters. Without that information, the resource agent cannot safely identify a bootstrap node and it won't tell Pacemaker to start the Galera cluster.&lt;/p&gt;
&lt;p&gt;The boot process works as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When a galera resource is in state &lt;em&gt;Started&lt;/em&gt;, the resource agent retrieves the last seqno from the local MariaDB, stores it in the CIB and goes to &lt;em&gt;Slave&lt;/em&gt; state. At this stage, no Galera server is running.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Once all the nodes are in &lt;em&gt;Slave&lt;/em&gt; state, the resource agent elects the bootstrap node, tags it in the CIB, and tells Pacemaker that it can promote the galera resource on this node to the &lt;em&gt;Master&lt;/em&gt; state.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When Pacemaker promotes the bootstrap node, the resource agent starts the Galera server, which bootstraps a new cluster. It then marks the remaining nodes as being ready for promotion. The resource on the bootstrap node is switched to &lt;em&gt;Master&lt;/em&gt;, and the Galera cluster is ready to accept SQL queries.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pacemaker promotes the remaining nodes. For each node, the resource agent start a Galera server, which synchronizes its local state with the cluster via a State Snapshot Transfer (SST). This operation can take some time. The promotion to &lt;em&gt;Master&lt;/em&gt; finishes when the synchronization is over and the Galera server is ready to accept SQL queries.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At this stage, the entire cluster is up and running, and the galera resource is set &lt;em&gt;Master&lt;/em&gt; on all nodes.&lt;/p&gt;
&lt;p&gt;Note: the notion of Master/Slave state is completely different from Galera's notion of Primary / Non-primary state&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Galera node is in primary state if it belongs to a partition of the cluster which has quorum (and is thus active)&lt;/li&gt;
&lt;li&gt;If a Galera node detects the partition it belongs to is inquorate, it will switch to Non-primary state, and SQL queries will fail&lt;sup id="fnref:2"&gt;&lt;a class="footnote-ref" href="#fn:2"&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Overriding the boot process&lt;/h2&gt;
&lt;p&gt;The resource agent expects all the nodes to be available for performing a boot. However, there are times where this is not the case and for practical reasons it is necessary to force the boot process.&lt;/p&gt;
&lt;p&gt;Here are examples of manual override scenarios, with steps to perform to bring the Galera cluster up. They apply on a  three-node Pacemaker cluster, composed of nodes &lt;code&gt;node1&lt;/code&gt;, &lt;code&gt;node2&lt;/code&gt;, &lt;code&gt;node3&lt;/code&gt;. In this Pacemaker cluster, the Galera resource is called &lt;code&gt;galeracluster&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Scenario 1: Galera cluster to be restarted, but one node won't come up&lt;/h3&gt;
&lt;p&gt;Suppose that &lt;code&gt;node3&lt;/code&gt; in the cluster is unavailable following an unexpected event (e.g. Galera crashed and left in a inconsistent state, hardware failure on &lt;code&gt;node3&lt;/code&gt;). In such case, the resource agent is not able to retrieve all &lt;code&gt;seqno&lt;/code&gt; in the cluster, so no bootstrap node can be elected, and cluster won't restart. One can force the election of a bootstrap node and start it, in order to unblock the resource agent and let Pacemaker boot the rest of the Galera cluster.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Do the following steps only if you're sure that the forced bootstrap node is up-to-date, otherwise you will permanently desynchronise your cluster and will lose data!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That being said, to unblock the boot process, you will need to elect and promote a bootstrap node manually. So first, take control of Galera away from Pacemaker:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; pcs resource unmanage galeracluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Next, identify the node with the most recent seqno. If Pacemaker previously tried to restart the cluster, you can retrieve this information in the CIB, e.g. for &lt;code&gt;node1&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; crm_attribute -N node1 -l reboot --name galeracluster-last-committed -Q
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If the last &lt;code&gt;seqno&lt;/code&gt; is not present in the CIB&lt;sup id="fnref:3"&gt;&lt;a class="footnote-ref" href="#fn:3"&gt;3&lt;/a&gt;&lt;/sup&gt;, you can retrieve it with MariaDB:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; mysqld_safe --wsrep-recover
&lt;span class="go"&gt;151002 13:59:35 mysqld_safe Logging to &amp;#39;/var/log/mariadb/mariadb.log&amp;#39;.&lt;/span&gt;
&lt;span class="go"&gt;151002 13:59:35 mysqld_safe Starting mysqld daemon with databases from /var/lib/mysql&lt;/span&gt;
&lt;span class="go"&gt;151002 13:59:35 mysqld_safe WSREP: Running position recovery with --log_error=&amp;#39;/var/lib/mysql/wsrep_recovery.2FkYLQ&amp;#39; --pid-file=&amp;#39;/var/lib/mysql/db1-recover.pid&amp;#39;&lt;/span&gt;
&lt;span class="go"&gt;151002 13:59:50 mysqld_safe WSREP: Recovered position 4c7ba2a8-566a-11e5-8250-1e939ac17c77:9&lt;/span&gt;
&lt;span class="go"&gt;151002 13:59:52 mysqld_safe mysqld from pid file /var/run/mariadb/mariadb.pid ended&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;MariaDB will recover its last known cluster position as &lt;code&gt;UUID:seqno&lt;/code&gt;. In our case, on &lt;code&gt;node1&lt;/code&gt; the last &lt;code&gt;seqno&lt;/code&gt; is thus &lt;code&gt;9&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once you determine which node has the bigger &lt;code&gt;seqno&lt;/code&gt;, make it the bootstrap node and force Pacemaker to start Galera by switching the resource's state to &lt;em&gt;Master&lt;/em&gt;. In our case, assuming &lt;code&gt;node1&lt;/code&gt; is the bootstrap node, connect to &lt;code&gt;node1&lt;/code&gt; and run the following commands locally:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; crm_attribute -N node1 -l reboot --name galeracluster-bootstrap -v &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; crm_attribute -N node1 -l reboot --name master-galeracluster -v &lt;span class="m"&gt;100&lt;/span&gt;
&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; crm_resource --force-promote -r galeracluster -V
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then, instruct Pacemaker to re-detect the current state of the galera resource. This will clean up failcount and purge knowledge of past failures:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; pcs resource cleanup galeracluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;At this point Galera is up and Pacemaker knows that it is up. Give back control of Galera to Pacemaker and the remaining node will join automatically&lt;sup id="fnref:4"&gt;&lt;a class="footnote-ref" href="#fn:4"&gt;4&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; pcs resource &lt;span class="nb"&gt;enable&lt;/span&gt; galeracluster
&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; pcs resource manage galeracluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Scenario 2: Multiple hardware failures, keep service on the remaining node&lt;/h3&gt;
&lt;p&gt;If &lt;code&gt;node2&lt;/code&gt; and &lt;code&gt;node3&lt;/code&gt; fail successively in the three-node cluster, you may end up with only &lt;code&gt;node1&lt;/code&gt; up and running. Pacemaker will react differently to this condition depending on how quorum is configured in the cluster&lt;sup id="fnref:5"&gt;&lt;a class="footnote-ref" href="#fn:5"&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;For Galera, things are less flexible: if two nodes out of three quit the cluster unexpectedly, the remaining node is considered inquorate and the Galera server will switch to Non-primary state. This is an error condition for the resource agent, and that causes Pacemaker to stop the Galera on the remaining node.&lt;/p&gt;
&lt;p&gt;You can force the restart of Galera on &lt;code&gt;node1&lt;/code&gt; if this node is still up and running in Pacemaker&lt;sup id="fnref:6"&gt;&lt;a class="footnote-ref" href="#fn:6"&gt;6&lt;/a&gt;&lt;/sup&gt;. You just need to bootstrap the Galera cluster by applying similar steps as those described in Scenario 1. &lt;strong&gt;Please only do so if you are sure that the node is in sync with the latest revision of the cluster, otherwise you will lose data&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Apply the step from Scenario 1 &lt;strong&gt;and stop before giving back control to Pacemaker&lt;/strong&gt;&lt;sup id="fnref:7"&gt;&lt;a class="footnote-ref" href="#fn:7"&gt;7&lt;/a&gt;&lt;/sup&gt;. At this point, check whether the Pacemaker cluster has quorum:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; corosync-quorumtool -s
&lt;span class="go"&gt;Quorum information&lt;/span&gt;
&lt;span class="go"&gt;------------------&lt;/span&gt;
&lt;span class="go"&gt;Date:             Fri Oct  2 18:20:37 2015&lt;/span&gt;
&lt;span class="go"&gt;Quorum provider:  corosync_votequorum&lt;/span&gt;
&lt;span class="go"&gt;Nodes:            1&lt;/span&gt;
&lt;span class="go"&gt;Node ID:          1&lt;/span&gt;
&lt;span class="go"&gt;Ring ID:          1376&lt;/span&gt;
&lt;span class="go"&gt;Quorate:          No&lt;/span&gt;

&lt;span class="go"&gt;Votequorum information&lt;/span&gt;
&lt;span class="go"&gt;----------------------&lt;/span&gt;
&lt;span class="go"&gt;Expected votes:   3&lt;/span&gt;
&lt;span class="go"&gt;Highest expected: 3&lt;/span&gt;
&lt;span class="go"&gt;Total votes:      1&lt;/span&gt;
&lt;span class="go"&gt;Quorum:           2 Activity blocked&lt;/span&gt;
&lt;span class="go"&gt;Flags:&lt;/span&gt;

&lt;span class="go"&gt;Membership information&lt;/span&gt;
&lt;span class="go"&gt;----------------------&lt;/span&gt;
&lt;span class="go"&gt;Nodeid      Votes Name&lt;/span&gt;
&lt;span class="go"&gt;     1          1 node1 (local)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If it doesn't, you have to unblock quorum temporarily for Pacemaker to manage resources, i.e. set the number of expected votes the the number of nodes which are still on-line. In our example, only &lt;code&gt;node1&lt;/code&gt; is on-line, so quorum can be temporarily unblocked with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; corosync-quorumtool -e1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that this setting is not definitive. As soon as other nodes rejoin, the number of expected votes will get back to the original value (3 in the example).&lt;/p&gt;
&lt;p&gt;Once the cluster is quorate again, you can give back control of Galera to Pacemaker:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="gp"&gt;[root@node1 ~]#&lt;/span&gt; pcs resource manage galeracluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;


&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn:1"&gt;
&lt;p&gt;Starting a new cluster can also be achieved with &lt;code&gt;--wsrep_new_cluster&lt;/code&gt;. The two options are equivalent.&amp;#160;&lt;a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:2"&gt;
&lt;p&gt;Data-related SQL queries will fail with &lt;code&gt;ERROR 1047 (08S01): WSREP has not yet prepared node for application use&lt;/code&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:3"&gt;
&lt;p&gt;If the information is not in the CIB, &lt;code&gt;crm_attribute&lt;/code&gt; will report an error like &lt;code&gt;Error performing operation: No such device or address&lt;/code&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:4"&gt;
&lt;p&gt;&lt;code&gt;pcs resource enable galeracluster&lt;/code&gt; will ensure that Pacemaker always try to promote this resource's state to &lt;em&gt;Master&lt;/em&gt;, i.e. start Galera server on the node if not already done.&amp;#160;&lt;a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:5"&gt;
&lt;p&gt;See &lt;code&gt;man votequorum&lt;/code&gt; and &lt;a href="http://clusterlabs.org/doc/en-US/Pacemaker/1.0/html/Pacemaker_Explained/s-cluster-options.html"&gt;no-quorum-policy settings&lt;/a&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:6"&gt;
&lt;p&gt;Check whether &lt;code&gt;node1&lt;/code&gt; is still online with &lt;code&gt;pcs status nodes&lt;/code&gt;.&amp;#160;&lt;a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn:7"&gt;
&lt;p&gt;Applying &lt;code&gt;pcs resource manage galeracluster&lt;/code&gt; will fail if the cluster is inquorate, and that will stop the Galera server that was manually restarted.&amp;#160;&lt;a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="Code"></category><category term="galera"></category><category term="openstack"></category></entry></feed>